{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1760012,"sourceType":"datasetVersion","datasetId":1046158}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dropout\n\n\ndf = pd.read_csv(\"/kaggle/input/crop-recommendation-dataset/Crop_recommendation.csv\")\n\nX = df.drop(\"label\", axis=1).values   \ny = df[\"label\"].values\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\ny_categorical = to_categorical(y_encoded)  \n\nimport pickle\nwith open(\"label_encoder.pkl\", \"wb\") as f:\n    pickle.dump(label_encoder, f)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nwith open(\"scaler.pkl\", \"wb\") as f:\n    pickle.dump(scaler, f)\n\nmodel = Sequential([\n    Dense(128, input_shape=(X_train.shape[1],), activation=\"relu\"),\n    Dropout(0.3),\n    Dense(64, activation=\"relu\"),\n    Dropout(0.3),\n    Dense(y_categorical.shape[1], activation=\"softmax\")\n])\n\n\n\nmodel.compile(optimizer=\"adam\",\n              loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])\n\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=100,\n    batch_size=16,\n    callbacks=[early_stop],\n    verbose=1\n)\n\nmodel.save(\"nn_model.h5\")\n\nprint(\"✅ Neural Network model trained and saved as nn_model.h5\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T07:04:17.147472Z","iopub.execute_input":"2025-09-13T07:04:17.148026Z","iopub.status.idle":"2025-09-13T07:04:32.922929Z","shell.execute_reply.started":"2025-09-13T07:04:17.148001Z","shell.execute_reply":"2025-09-13T07:04:32.922208Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.1593 - loss: 2.8382 - val_accuracy: 0.6545 - val_loss: 1.6581\nEpoch 2/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5848 - loss: 1.5213 - val_accuracy: 0.8273 - val_loss: 0.7240\nEpoch 3/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7287 - loss: 0.8452 - val_accuracy: 0.8909 - val_loss: 0.4471\nEpoch 4/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7841 - loss: 0.6484 - val_accuracy: 0.9364 - val_loss: 0.3261\nEpoch 5/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8134 - loss: 0.5209 - val_accuracy: 0.9341 - val_loss: 0.2679\nEpoch 6/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8567 - loss: 0.4369 - val_accuracy: 0.9386 - val_loss: 0.2099\nEpoch 7/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8400 - loss: 0.4236 - val_accuracy: 0.9386 - val_loss: 0.2033\nEpoch 8/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8770 - loss: 0.3365 - val_accuracy: 0.9500 - val_loss: 0.1639\nEpoch 9/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8899 - loss: 0.3203 - val_accuracy: 0.9409 - val_loss: 0.1601\nEpoch 10/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8947 - loss: 0.2757 - val_accuracy: 0.9568 - val_loss: 0.1320\nEpoch 11/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9085 - loss: 0.2739 - val_accuracy: 0.9523 - val_loss: 0.1320\nEpoch 12/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9154 - loss: 0.2554 - val_accuracy: 0.9545 - val_loss: 0.1227\nEpoch 13/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9155 - loss: 0.2551 - val_accuracy: 0.9545 - val_loss: 0.1089\nEpoch 14/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9253 - loss: 0.2013 - val_accuracy: 0.9568 - val_loss: 0.1090\nEpoch 15/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9224 - loss: 0.1935 - val_accuracy: 0.9614 - val_loss: 0.1002\nEpoch 16/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9258 - loss: 0.2092 - val_accuracy: 0.9727 - val_loss: 0.0876\nEpoch 17/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9455 - loss: 0.1601 - val_accuracy: 0.9682 - val_loss: 0.0824\nEpoch 18/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9336 - loss: 0.1810 - val_accuracy: 0.9705 - val_loss: 0.0803\nEpoch 19/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9425 - loss: 0.1621 - val_accuracy: 0.9659 - val_loss: 0.0820\nEpoch 20/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9318 - loss: 0.1917 - val_accuracy: 0.9705 - val_loss: 0.0788\nEpoch 21/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9353 - loss: 0.1874 - val_accuracy: 0.9727 - val_loss: 0.0741\nEpoch 22/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9424 - loss: 0.1693 - val_accuracy: 0.9750 - val_loss: 0.0746\nEpoch 23/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9530 - loss: 0.1507 - val_accuracy: 0.9795 - val_loss: 0.0634\nEpoch 24/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9387 - loss: 0.1571 - val_accuracy: 0.9818 - val_loss: 0.0608\nEpoch 25/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9499 - loss: 0.1389 - val_accuracy: 0.9727 - val_loss: 0.0639\nEpoch 26/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9537 - loss: 0.1245 - val_accuracy: 0.9727 - val_loss: 0.0711\nEpoch 27/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9392 - loss: 0.1681 - val_accuracy: 0.9795 - val_loss: 0.0625\nEpoch 28/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9427 - loss: 0.1459 - val_accuracy: 0.9727 - val_loss: 0.0662\nEpoch 29/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.1371 - val_accuracy: 0.9773 - val_loss: 0.0547\nEpoch 30/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9481 - loss: 0.1349 - val_accuracy: 0.9773 - val_loss: 0.0543\nEpoch 31/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9621 - loss: 0.1037 - val_accuracy: 0.9909 - val_loss: 0.0456\nEpoch 32/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9511 - loss: 0.1362 - val_accuracy: 0.9841 - val_loss: 0.0511\nEpoch 33/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9649 - loss: 0.1047 - val_accuracy: 0.9727 - val_loss: 0.0568\nEpoch 34/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9517 - loss: 0.1278 - val_accuracy: 0.9818 - val_loss: 0.0508\nEpoch 35/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9654 - loss: 0.1047 - val_accuracy: 0.9886 - val_loss: 0.0467\nEpoch 36/100\n\u001b[1m110/110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1292 - val_accuracy: 0.9773 - val_loss: 0.0646\n✅ Neural Network model trained and saved as nn_model.h5\n","output_type":"stream"}],"execution_count":5}]}